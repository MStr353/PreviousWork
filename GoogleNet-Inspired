#Compressed data

from sklearn.metrics import mean_squared_error, r2_score
from math import sqrt
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn import *
import os
import numpy as np
from PIL import Image

# Define the directories
root_dirs = ['IR500', 'XR750']
sub_dirs = ['500_1_2_255', '500_1_2_520', '500_6_8_255', '500_6_8_520', '500Water255', '500Water520', 
            '750_1_2_255', '750_1_2_520', '750_6_8_255', '750_6_8_520', '750Water255', '750Water520']
time_points = ['000', '005', '010', '015', '030', '045', '060', '075', '120', '180', '240', '300', '360', '420', '480', '540', '600', '660', '720']

# Define a dictionary to store the data
dataset_dict = {}

# Loop over the directories and time points
for root_dir in root_dirs:
    for sub_dir in sub_dirs:
        for time in time_points:
            # Construct the file path
            filepath = os.path.join(root_dir, sub_dir, f'frame{time}min.tif')
            if os.path.exists(filepath):
                with Image.open(filepath) as img:
                    # Resize the image
                    img_resized = img.resize((img.size[0] // 10, img.size[1] // 10))
                    # Convert the image to a numpy array and normalize pixel values
                    img_array = np.array(img_resized) / 255.0
                    # Define the key for this image in the dictionary
                    key = (root_dir, time)
                    # If the key is not already in the dictionary, add it with an empty list as the value
                    if key not in dataset_dict:
                        dataset_dict[key] = []
                    # Append the image data to the list for this key
                    dataset_dict[key].append((img_array, filepath))


# Create a new list for the dataset
dataset_cnn2 = []

# Define the order of root directories and time points
root_dirs_ordered = ['IR500', 'XR750']
time_points_ordered = ['000', '005', '010', '015', '030', '045', '060', '075', 
                       '120', '180', '240', '300', '360', '420', '480', '540', '600', '660', '720']

# Loop over each root directory and time point in the specified order
for root_dir in root_dirs_ordered:
    for time_point in time_points_ordered:
        # If this (root directory, time point) pair is in the dictionary, stack the images
        if (root_dir, time_point) in dataset_dict:
            # Get the list of image tuples for this root directory and time point
            image_tuples = dataset_dict[(root_dir, time_point)]
            # Extract just the image data for stacking
            image_stack = np.stack([img for img, filepath in image_tuples])
            # Add the stacked images to the new dataset
            dataset_cnn2.append(image_stack)

# dataset_cnn is now a list of 3D numpy arrays, each representing a stack of images for a particular time point for a particular tablet

# Load the CSV file
data = pd.read_csv('FinalDataFile.csv')

# Get the labels
labels = data['InVivo'].values.reshape(-1,1)

scalerY=preprocessing.MinMaxScaler(feature_range=(0, 1))
scalerY.fit(labels)
Y_scaled=pd.DataFrame(scalerY.transform(labels.reshape(-1,1))) 

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Conv3D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import MaxPooling3D
from tensorflow.keras.layers import concatenate
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import plot_model
import gc
# Define your dataset and targets

dataset = dataset_cnn2
X_shape = dataset[0].shape
Y = Y_scaled
# Get the shape of the images
input_shape = dataset_cnn2[0].shape

# function for creating a projected inception module
def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):
    # 1x1 conv
    conv1 = Conv3D(f1, (1,1,1), padding='same', activation='relu')(layer_in)
    # 3x3 conv
    conv3 = Conv3D(f2_in, (1,1,1), padding='same', activation='relu')(layer_in)
    conv3 = Conv3D(f2_out, (3,3,3), padding='same', activation='relu')(conv3)
    # 5x5 conv
    conv5 = Conv3D(f3_in, (1,1,1), padding='same', activation='relu')(layer_in)
    conv5 = Conv3D(f3_out, (5,5,5), padding='same', activation='relu')(conv5)
    # 3x3 max pooling
    pool = MaxPooling3D((3,3,3), strides=(1,1,1), padding='same')(layer_in)
    pool = Conv3D(f4_out, (1,1,1), padding='same', activation='relu')(pool)
    # concatenate filters, assumes filters/channels last
    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)
    return layer_out

def final_module(layer_in):
    Flat = Flatten()(layer_in)
    
    Drop = Dropout(0.3)(Flat)
    
    Dense1 = Dense(64, activation='relu')(Drop)
    
    Dense2 = Dense(64, activation='relu')(Dense1)
    
    layer_out = Dense(1, activation='linear')(Dense2)
    return layer_out

def create_model():
    # define model input
    visible = Input(shape=X_shape)
    # add inception block 1
    layer = inception_module(visible, 64, 96, 128, 16, 32, 32)
    # add inception block 2
    layer = inception_module(layer, 128, 128, 192, 32, 96, 64)
    #adding final layer
    layer = final_module(layer)
    # create model

#def create_model():
    model = Model(inputs=visible, outputs=layer)

    # summarize model
    #model.summary()
    # plot model architecture
    #plot_model(model, show_shapes=True, to_file='inception_module.png')
    # Compile the model
    model.compile(loss='mse', optimizer='adam', metrics='mse')
    return model

# Define the number of folds for the cross-validation
learning_time=[1,10,20,30,40,50,100]
n_folds = 10
kfold = KFold(n_folds, shuffle=True, random_state=1)

for i in learning_time:
    # Initialize lists to store predictions and true values
    all_predictions = []
    all_true_values = []

    # Iterate over each fold
    for train, test in kfold.split(dataset):
        # Split the data into training and test sets for this fold
        X_train, X_test = np.array(dataset)[train], np.array(dataset)[test]
        y_train, y_test = np.array(Y)[train], np.array(Y)[test]

        # Create a new model for this fold
        model = create_model()

        # Train the model
        model.fit(X_train, y_train, epochs=i, verbose=True)

        # Make predictions
        predictions = model(X_test)

        # Append the predictions and the true values to the lists
        all_predictions.append(predictions)
        all_true_values.append(y_test)
        gc.collect()
    # Convert the lists to numpy arrays
    all_predictions = np.concatenate(all_predictions)
    all_true_values = np.concatenate(all_true_values)

    # Unscale the predictions and the true values
    all_predictions_unscaled = scalerY.inverse_transform(all_predictions)
    all_true_values_unscaled = scalerY.inverse_transform(all_true_values)

    # Calculate RMSE and R^2
    rmse = sqrt(mean_squared_error(all_true_values_unscaled, all_predictions_unscaled))
    r2 = r2_score(all_true_values_unscaled, all_predictions_unscaled)

    with open(f'Results_GoogleNet_{i}iterations.txt', 'w') as f:
        f.write(f'RMSE: {rmse}\n')
        f.write(f'R^2: {r2}\n')
        f.write('Predictions:\n')
        f.write(np.array2string(all_predictions_unscaled))
        f.write('\nTrue values:\n')
        f.write(np.array2string(all_true_values_unscaled))
